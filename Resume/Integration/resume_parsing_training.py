# -*- coding: utf-8 -*-
"""Resume Parsing - Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11rwJtbOBjo0jk2aZr-WFMKavia36VEcC

# Resume Parsing
Version 3.0

# Imports
"""

!pip install PyMuPDF
!pip install docx2pdf
import pandas as pd
import numpy as np
import sys
import fitz
from docx2pdf import convert
import nltk
import string
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from collections import Counter

# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')

"""# Kaggle API"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d snehaanbhawal/resume-dataset

!unzip resume-dataset.zip

"""# Pre-processing
* It consists of some main steps
    * Lowercase
    * Removing Punctuation
    * Tokenization
    * Stopword Filtering
    * Stemming
    * Lemmatization

## Cleaning
"""

def clean(df):
  resume_df = df.copy()
  resume_df[['ID']] = resume_df[['ID']].apply(pd.to_numeric, errors='coerce')
  resume_df.drop(columns=['Resume_html'], inplace = True)
  return resume_df

"""## Lowercase"""

def to_lower(df):
  lower_df = df.copy()
  lower_df["Resume_str"] = lower_df["Resume_str"].str.lower()
  lower_df["Category"] = lower_df["Category"].str.lower()
  return lower_df

"""## Removing Punctuation"""

def rem_punct(df):
  punct_df = df.copy()
  punct_df['punct_sent'] = punct_df.apply(lambda row: "".join([char for char in row['Resume_str'] if char not in string.punctuation]), axis=1)
  return punct_df

"""## Tokenization"""

def to_tokens(df):
  tokens_df = df.copy()
  tokens_df['tokenized_sents'] = tokens_df.apply(lambda row: nltk.word_tokenize(row['punct_sent']), axis=1)
  return tokens_df

"""## Stop Words"""

def rem_stop_words(df):
  stop_df = df.copy()
  stop_words = stopwords.words('english')
  stop_df['stop_words'] = stop_df.apply(lambda row: [word for word in row['tokenized_sents'] if word not in stop_words], axis=1)
  return stop_df

"""## Stemming"""

def stemming(df):
  new_df = df.copy()
  porter = PorterStemmer()
  new_df['Stemmed'] = df.apply(lambda row:[porter.stem(word) for word in row['stop_words']], axis = 1)
  return new_df

"""## Lemmatization"""

def lemmatizing(df):
  new_df = df.copy()
  lemmatizer = WordNetLemmatizer()
  new_df['lemmatized'] = df.apply(lambda row:[lemmatizer.lemmatize(word) for word in row['stop_words']], axis = 1)
  return new_df

"""# Feature Extraction

## TF
"""

def TF_doc(df):
  TF_document_df = df.copy()
  #### Retrun to stemming / lemmetization
  TF_document_df['TF_doc'] = TF_document_df.apply(lambda row: Counter(row['stop_words']), axis=1)
  return TF_document_df

"""## TF for each Category"""

def TF_category(df):
  ## Getting the Categories Names
  TF_cat = pd.DataFrame(
                  columns=pd.Index( df.Category.unique()),
                  index=pd.Index([]))
  ## Calculating words frequency within each category
  for index, row in df.iterrows():
    for item, value in row["TF_doc"].items():
      if item not in TF_cat.index:
        TF_cat.loc[item] = 0
        TF_cat.loc[item, row["Category"]] = value
      else:      
        TF_cat.loc[item, row["Category"]] += value

  return TF_cat

def TF_Normalize(df):
  TF_Normalized = df.copy()
  for col in TF_Normalized:
    TF_Normalized[col]/=TF_Normalized[col].sum()
  return TF_Normalized

"""## IDF"""

def IDF(df):
  # Make a Copy to work with
  IDF_DF = df.copy()
  
  # 1+ loge(No of documents in corpus/No. of documents containing the word)
  IDF_DF['IDF'] = IDF_DF.apply(lambda row: 1+ np.log(len(IDF_DF.columns)/row.astype(bool).sum()), axis=1)

  return IDF_DF

"""## TF-IDF"""

def TF_IDF(TF, IDF):
  TF_IDF_DF = TF.copy()
  TF_IDF_DF = TF_IDF_DF.multiply(IDF["IDF"], axis="index")
  return TF_IDF_DF

"""# Main"""

def main(file_name):
  resume_df = pd.read_csv(file_name)
  cleaned_df = clean(resume_df)
  lowered_df = to_lower(cleaned_df)
  punct_df = rem_punct(lowered_df)
  tokenized_df = to_tokens(punct_df)
  stop_words_df = rem_stop_words(tokenized_df)
  stemmed_df = stemming(stop_words_df)
  lemma_df = lemmatizing(stemmed_df)
  TF_doc_df = TF_doc(lemma_df)
  TF_cat_df = TF_category(TF_doc_df)
  TF_Norm_df = TF_Normalize(TF_cat_df)
  IDF_df = IDF(TF_cat_df)
  TF_IDF_df = TF_IDF(TF_Norm_df, IDF_df)

  return TF_IDF_df

train_result = main("Resume/Resume.csv")

train_result

train_result.to_csv("train_tf_idf.csv")

"""# Debugging"""

resume_df = pd.read_csv("Resume/Resume.csv")
resume_df

resume_df.describe()

cleaned_df = clean(resume_df)
cleaned_df

lowered_df = to_lower(cleaned_df)
lowered_df

punct_df = rem_punct(lowered_df)
punct_df

tokenized_df = to_tokens(punct_df)
tokenized_df

stop_words_df = rem_stop_words(tokenized_df)
stop_words_df

stemmed_df = stemming(stop_words_df)
stemmed_df

lemma_df = lemmatizing(stemmed_df)
lemma_df

TF_doc_df = TF_doc(stop_words_df)
TF_doc_df

TF_cat_df = TF_category(TF_doc_df)
TF_cat_df

TF_cat_df.to_csv("train.csv")

TF_Norm_df = TF_Normalize(TF_cat_df)
TF_Norm_df

IDF_df = IDF(TF_cat_df)
IDF_df

TF_IDF_df = TF_IDF(TF_Norm_df, IDF_df)
TF_IDF_df

TF_IDF_df.to_csv("TF_IDF.csv")



