{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Screening\n",
    "Version 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Pipeline\n",
    "\n",
    "1. Resume Parsing  \n",
    "    * The first subtask is parsing the resume, i.e., extracting information in a structured format from the document. \n",
    "2. Resume Matching\n",
    "    * The second sub-task is extracting semantic information and actually understanding the underlying information.\n",
    "3. Database Formation\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF\n",
    "# !pip install docx2pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import fitz\n",
    "from docx2pdf import convert\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    if fname.split(\".\")[1] == 'docx':\n",
    "        convert(fname)\n",
    "        fname = fname.split(\".\")[0] + \".pdf\"\n",
    "    elif fname.split(\".\")[1] == 'pdf':\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Only PDF and docx types are supported!\")\n",
    "        return\n",
    "    \n",
    "    doc = fitz.open(fname)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text = text + str(page.get_text())\n",
    "\n",
    "    tx = \" \".join(text.split('\\n'))  \n",
    "    #print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = read_file('Ahmed Osama-Long Version.docx')\n",
    "resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "* It consists of some main steps\n",
    "    * Lowercase\n",
    "    * Removing Punctuation\n",
    "    * Tokenization\n",
    "    * Stopword Filtering\n",
    "    * Stemming\n",
    "    * Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_lower = resume.lower()\n",
    "resume_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(string.punctuation)\n",
    "## Note + needs to be kept -> C++\n",
    "## Note @ needs to be kept -> May remove emails \n",
    "## Remove bullet points\n",
    "## Remove Qoutes\n",
    "resume_punc = \"\".join([char for char in resume_lower if char not in string.punctuation + \"●•\"])\n",
    "print(resume_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_tokens = nltk.word_tokenize(resume_punc)\n",
    "resume_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "#print(stop_words)\n",
    "resume_filtered_words = [word for word in resume_tokens if word not in stop_words]\n",
    "#print(resume_filtered_words)\n",
    "resume_filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "stemmed_resume = [porter.stem(word) for word in resume_filtered_words]\n",
    "print(stemmed_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Count (Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ahm': 1, 'osama': 1, 'moham': 1, 'address': 1, 'giza': 1, 'egypt': 1, 'mobil': 1, '201118862871': 1, 'email': 1, 'ahmadosgalalgmailcom': 1, 'linkedin': 1, 'linkedincominahmaddosama': 1, 'github': 1, 'githubcomahmadosgal': 1, 'hackerrank': 1, 'hackerrankcomahmadgalal99': 1, 'work': 1, 'experi': 1, 'softwar': 1, 'develop': 3, 'internship': 1, 'shadi': 1, 'system': 9, 'summer': 1, '2021': 3, 'implement': 6, 'databas': 3, 'queri': 3, 'store': 3, 'procedur': 1, 'build': 1, 'web': 4, 'applic': 2, 'call': 1, '“': 3, 'smart': 1, '”': 3, 'use': 10, 'sql': 4, 'microsoft': 1, 'server': 1, 'postman': 1, 'api': 2, 'test': 2, 'aspnet': 1, 'core': 1, 'document': 2, 'featur': 2, 'educ': 2, 'certif': 1, 'comput': 2, 'scienc': 1, 'cairo': 1, 'univers': 1, 'faculti': 1, 'engin': 4, 'credit': 1, 'hour': 1, '2017': 2, '–': 3, '2022': 1, 'commun': 1, 'ccec': 1, 'cumul': 1, 'gpa': 1, '378400': 1, 'secondari': 1, 'dar': 1, 'el': 1, 'tarbiah': 1, 'school': 1, '2014': 1, 'technolog': 2, 'languag': 3, 'c': 5, 'java': 3, 'python': 2, 'full': 2, 'stack': 1, 'html': 1, 'css': 1, 'laravel': 2, 'node': 1, 'js': 1, 'spring': 2, 'boot': 1, 'mysql': 1, 'mongo': 1, 'git': 2, 'rto': 1, 'machin': 2, 'learn': 3, 'data': 3, 'analysi': 1, 'sklearn': 1, 'panda': 1, 'numpi': 1, 'matplotlib': 1, 'neural': 1, 'network': 1, 'deep': 1, 'tensorflow': 1, 'kera': 1, 'trax': 1, 'nlp': 2, 'word': 1, 'embed': 1, 'probabilist': 1, 'model': 5, 'sequenc': 1, 'attent': 1, 'structur': 1, 'algorithm': 2, 'objectori': 1, 'program': 1, 'extracurricular': 1, 'activ': 1, 'jan': 1, 'present': 1, 'member': 1, 'beat': 1, 'technic': 1, 'team': 2, 'june': 1, 'stp': 1, 'machathon': 2, '20': 2, 'arab': 2, 'competit': 2, 'finalist': 1, '2nd': 1, 'place': 2, 'project': 3, 'facial': 1, 'recognit': 3, 'consist': 1, '3': 1, 'modul': 2, 'face': 2, 'detect': 2, 'landmark': 1, 'differ': 2, 'techniqu': 1, 'like': 2, 'eigen': 1, 'local': 1, 'binari': 1, 'pattern': 1, 'histogram': 1, 'lbph': 1, 'ticket': 1, 'book': 2, 'built': 1, 'fulli': 1, 'function': 2, 'framework': 1, 'backend': 1, 'vuej': 1, 'frontend': 1, 'search': 1, 'bootreactmysql': 1, 'multithread': 1, 'crawler': 1, 'index': 1, 'output': 1, 'creat': 1, 'processor': 3, 'interfac': 1, 'nodej': 2, 'react': 1, 'music': 1, 'player': 2, 'spotifi': 3, 'clone': 2, 'construct': 1, 'rest': 1, 'artist': 1, 'expressj': 1, 'design': 1, 'nosql': 1, 'mongodb': 1, 'mongoos': 1, 'practic': 1, 'unit': 1, 'well': 1, 'version': 1, 'control': 1, 'egyptian': 1, 'chatbot': 2, 'messag': 1, 'intent': 1, 'classif': 1, 'solv': 1, 'final': 1, 'dataset': 2, 'second': 1, 'writer': 2, 'identif': 1, 'identifi': 1, 'scratch': 1, 'tri': 1, 'svm': 1, 'knn': 1, 'train': 1, 'iam': 1, 'accuraci': 1, '987': 1, 'footbal': 2, 'scout': 3, 'accomplish': 1, 'entri': 1, 'virtualo': 1, 'variou': 1, 'process': 1, 'schedul': 1, 'associ': 1, 'memori': 1, 'manag': 1, 'simul': 1, 'task': 1, 'oper': 1, '5stage': 2, 'pipelin': 2, 'simpl': 1, 'base': 1, 'harvard': 1, 'architectur': 1, 'vhdl': 1, 'nativ': 1, 'english': 1, 'profici': 1}\n"
     ]
    }
   ],
   "source": [
    "resume_freq = dict()\n",
    "for word in stemmed_resume:\n",
    "    resume_freq[word] = resume_freq.get(word,0)+1\n",
    "print(resume_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
